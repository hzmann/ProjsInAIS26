{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLdYQCxEmiRA0gya7DgYdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hzmann/ProjsInAIS26/blob/main/Proj_in_ai_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "YTGCQFrfL3w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. Derive the objective function for Logistic Regression using Maximum Likelihood\n",
        "Estimation (MLE). Do some research on the MAP technique for Logistic Regression,\n",
        "include your research on how this technique is different from MLE (include citations)**"
      ],
      "metadata": {
        "id": "twAj_-ItL9A0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Goal:** given n training samples, each training sample i, each feature vector $x_i$, and each label $y_i$ (either 1 or 0),\n",
        "      find the parameters w and bias b that make the observed labels most likely  \n",
        "\n",
        "  \n",
        "**First compute a linear score and apply sigmoid function to map real inputs to probabilities in [0,1]--  \n",
        " Score:**\n",
        "$$\n",
        "z_i = w^Tx_i + b = \\sum_{j=1}^{d}w_jx_{ij} + b\n",
        "$$  \n",
        "\n",
        "**Convert score to a probability using the sigmoid function, which maps scores to the interval [0,1] so output can be interpreted as a probability:**\n",
        "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
        "**So for each sample i, the model's predicted probability for i is:**\n",
        "$$p_i = P(y_i = 1 | x_i,w,b) = \\sigma(z_i)$$\n",
        "$$\n",
        "-->\\sigma(w^Tx_i + b)$$\n",
        "**Probabilities must sum to 1 for a binary outcome, so**\n",
        "$$ P(y_i = 0 | x_i,w,b) = 1 - p_i $$  \n",
        "**Since each label $y_i$ is binary, it can be modeled as a Bernoulli random variable with parameter $p_i$--**\n",
        "**Probability of observing one $y_i$:**\n",
        "$$ P(y_i | x_i, w, b) = p_i^{y_i}(1-p_i)^{1-y_i} $$\n",
        "**Assumption: all examples are independent, so likelihood function is:**\n",
        "$$L(w,b) = \\prod_{i=1}^{n}P(y_i | x_i, w, b) = p_i^{y_i}(1-p_i)^(1-y_i)$$\n",
        "**To choose the parameters that maximize the likelihood, we use (w, b) = argmaxL(w,b)**\n",
        "**Products are hard to work with so we will turn the product into a sum using log-likelihood:**\n",
        "$$ \\ell(w,b) = \\log{L(w,b)}$$\n",
        "$$--> L(w,b) = \\prod_{i=1}^{n}p_i^{y_i}(1-p_i)^{1-y_i}$$\n",
        "**Take log of both sides:**\n",
        "$$--> \\ell(w,b) = \\log(\\prod_{i=1}^{n}p_i^{y_i}(1-p_i)^{1-y_i})$$\n",
        "**Use the rule $\\log (\\prod_{i=1}^{n}x_i) = \\sum_{i=1}^{n} \\log(x_i)$:**\n",
        "$$--> \\ell(w,b) = \\sum_{i = 1}^{n}(\\log(p_i^{y_i}(1-p_i)^{1-yi}))$$\n",
        "**Use the log rule $log(ab) = loga + logb:$**\n",
        "$$--> \\ell(w,b) = \\sum_{i = 1}^{n}(\\log(p_i^{y_i}) + \\log((1-p_i)^{1-y_i}))$$\n",
        "**Use the log rule $\\log(a^c)$ = $c\\log(a)$:**\n",
        "$$--> \\ell(w,b) = y_i\\log(p_i) + (1-y_i)\\log(1-p_i)$$\n",
        "with $p_i = σ(w^Tx_i + b)$  \n",
        "\n",
        "**Turn optimizing into minimizing loss-- minimizing the negative log-likelihood is equivalent to maximizing the log-likelihood:**\n",
        "$$max\\ell(w,b) --> min -l(w,b)$$\n",
        "**NLL: $N(w,b) = -l(w,b)$**  \n",
        "\n",
        "# ***Objective function for LR:***\n",
        "$$N(w,b) = -\\sum_{i = 1}^{n} (y_i\\log(\\sigma(w^Tx_i + b)) + (1-y_i)\\log(1-\\sigma(w^Tx_i + b)))$$\n",
        "\n",
        "# MAP (Maximum a Posteriori) technique\n",
        "- instead of just picking the model that fits the data best, we pick the model that fits the data well and looks reasonable based on assumptions we make about good parameter values before we even see the data\n",
        "- MLE tells us to choose parameters that make the observed data as likely as possible\n",
        "  - for datasets that are too small or noisy, that can mean MLE is trying to fit randomness or overfit badly\n",
        "- MAP incorporates prior beliefs about what parameter values are reasonable (called priors)\n",
        "  - this tells us very large weights are unlikely, simpler models are more plausible, and parameters should be near zero unless clear evidence of otherwise\n",
        "- these priors are mathematically encoded as $P(\\theta)$\n",
        "  - they penalize overly complex or extreme models\n",
        "  - reduces overconfidence by leaning on the priors\n",
        "  - so MAP = MLE + regularization\n",
        "- posterior: $P(\\theta|D)$\n",
        "  - how plausible the paramaters are after seeing the data\n",
        "  - MAP maximizes the posterior\n",
        "  - $P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$, or  $P(\\theta|D) ∝ P(D|\\theta)P(\\theta)$ because P(D) is a constant that does not depend on $\\theta$\n",
        "- thus, MLE works better for large datasets while MAP is the better choice for smaller datasets  \n",
        "\n",
        "**citations**\n",
        "- https://stats.stackexchange.com/questions/95898/mle-vs-map-estimation-when-to-use-which#:~:text=Here's%20some%20information%20about%20when%20to%20use,summarize%20the%20posterior%20distribution%20*%20Reparameterization%20invariance\n",
        "- https://www.geeksforgeeks.org/data-science/mle-vs-map/\n",
        "\n"
      ],
      "metadata": {
        "id": "nX7Z16czhX-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Define a machine learning problem you wish to solve using Logistic Regression. Justify\n",
        "why logistic regression is the best choice and compare it briefly to another linear\n",
        "classification model**  \n",
        "  \n",
        "  The problem I wish to solve with Logistic Regression is the probability an individual has heart disease given health measurement features.\n",
        "\n",
        "  Logistic Regression is a strong choice here because the problem can clearly be modeled by P(heart disease = 1 | patient features), with risk estimates/probabilities being more useful in healthcare than hard yes or no classifications (we want a probability, not just a label). The heart disease outcome is also naturally binary, with the outcome being the probability of having heart disease vs. not having heart disease which fits the Bernoulli distribution of logistic regression.  \n",
        "\n",
        "  Other linear models, such as SVM, are not as well suited to this problem. This is because we want to model calibrated probabilities, but instead, SVM learns a boundary that separates the classes with maximal margin; it answers where the line should be drawn so the at-risk and not at-risk groups are farthest apart, not how likely it is for a patient to have heart disease. Also, while two patients may be classified by SVM as being on the same side of that line (say, both at risk of heart disease) one may be at a far greater risk than the other. Class membership gives nothing about risk, and this is an important difference that would be lost."
      ],
      "metadata": {
        "id": "msUzaHAxDlHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**c. Discuss how your dataset corresponds to the variables in your equations, highlighting\n",
        "any assumptions in your derivation from part 1**  \n",
        "\n",
        "The dataset I have chosen is the Heart Disease Dataset from Kaggle: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset.  \n",
        "\n",
        "In part 1, I used the variables $x_i$ for feature vectors, $y_i$ for labels, w and b for parameters, and $p_i = P(y_i = 1|x_i, w, b)$ for predicted probabilities. There are 13 features: age, sex, cp (chest pain type, 1 of 4 values), trestbps (resting blood pressure), chol (cholesterol in mg), fbs (fasting blood pressure), restecg (resting electrocardiographic results, values 0,1, or 2), thalach (max heart rate achieved), exang (exercise induced angina), oldpeak (ST depression induced by exercise), slope (the slope of the peak exercise ST segment), ca (number of major vessels 0-3 colored by flouroscopy), and thal (0 = normal; 1 = fixed defect; 2 = reversible defect). Each row of the dataset corresponds to one patient, ie, one $i$. Thus, each patient $i$ has feature vector $x_i = x_{i1...13}$. Variable $y_i$ comes from the target column, with values 1 for the patient having heart disease and 0 for not having heart disease. The model parameters w and b will be the learned weight w per feature and bias term b, which will define the linear score $z_i = w^Tx_i + b$. The assumption for the derivation is the model $p_i = P(y_i = 1 | x_i, w, b)$, which in the context of this dataset, means: given a patient's health information, the model predicts the probability that the patient has heart disease.  \n",
        "\n",
        "Assumptions:  \n",
        "We assume each $y_i$ is a Bernoulli random variable, which holds up because each $y_i$ in the dataset is either 0 or 1. We also assume training examples are independent, which is true because the diagnoses of each patient is independent of one another. We also assume the log-odds of a prediction is a linear function of its features; however, in real human bodies, biological systems are more complex and may behave in nonlinear ways. Health measurements can vary in inconsistent ways, can interact with one another, and may not affect all patients equally. Despite these disclaimers, this simplifying assumption is often reasonable and effective for logistic regression. Finally, we implicitly assume all values used in the derivation are meaningful, but in this real world dataset, there are some missing values we will have to preprocess.\n"
      ],
      "metadata": {
        "id": "k3hI4h2qFO3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "SClYH8qZRK2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a. The dataset I have chosen is the Heart Disease Dataset from Kaggle: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset. It has 13 features."
      ],
      "metadata": {
        "id": "TxNOGGr1RRBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**b. Perform Exploratory Data Analysis (EDA), addressing potential multicollinearity among\n",
        "features. Use Variance Inflation Factor (VIF) to identify highly correlated variables and\n",
        "demonstrate steps to handle them.***"
      ],
      "metadata": {
        "id": "BmK5Jz-3bJ8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# load dataset\n",
        "df = pd.read_csv(\"C:/ProjAI/heart.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v2ySNoTqcR7L",
        "outputId": "6dabda3a-626a-4b99-8a11-ee96d7fa9808"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:9: SyntaxWarning: invalid escape sequence '\\P'\n",
            "<>:9: SyntaxWarning: invalid escape sequence '\\P'\n",
            "/tmp/ipython-input-2934969269.py:9: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  df = pd.read_csv(\"C:\\ProjAI\\heart.csv\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\ProjAI\\\\heart.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2934969269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C:\\ProjAI\\heart.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\ProjAI\\\\heart.csv'"
          ]
        }
      ]
    }
  ]
}